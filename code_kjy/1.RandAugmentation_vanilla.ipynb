{"cells":[{"cell_type":"markdown","id":"TLXVT-jnspeq","metadata":{"id":"TLXVT-jnspeq"},"source":["## RandAugmentation_vanila\n","- baseline code가 존재하는 디렉토리에 해당 노트북을 다운받아 실행해주시기바랍니다."]},{"cell_type":"code","execution_count":1,"id":"5919106d-53e4-4e07-ad0a-ef9e3a04d3b3","metadata":{"id":"5919106d-53e4-4e07-ad0a-ef9e3a04d3b3"},"outputs":[],"source":["import torch\n","import os\n","import sys\n","import yaml\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import DataLoader, random_split\n","from typing import Tuple\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","if \"./\" not in sys.path:\n","    sys.path.append(\"./\")"]},{"cell_type":"code","execution_count":2,"id":"d0ab232d-8273-4091-8fdf-fd946b1cdfa6","metadata":{"id":"d0ab232d-8273-4091-8fdf-fd946b1cdfa6"},"outputs":[],"source":["from src.model import Model\n","from src.trainer import TorchTrainer\n","from src.loss import CustomCriterion"]},{"cell_type":"code","execution_count":3,"id":"830e4980-fbb3-4113-9537-868b21003650","metadata":{"id":"830e4980-fbb3-4113-9537-868b21003650"},"outputs":[],"source":["MEAN_V = (0.4914, 0.4822, 0.4465)\n","STD_V = (0.2470, 0.2435, 0.2616)\n","\n","DATASET_DIR = \"./input/cifar10\""]},{"cell_type":"code","execution_count":4,"id":"8bf58e84-3317-4311-b9bb-b1ae570b62f7","metadata":{"id":"8bf58e84-3317-4311-b9bb-b1ae570b62f7"},"outputs":[],"source":["def generate_transform(resize: int = 32, aug_fcns: Tuple = ()) -> transforms.transforms.Compose:\n","    \"\"\"Generate train augmentation policy.\"\"\"\n","    transform_fcns = []\n","    transform_fcns.append(transforms.Resize((resize, resize)))\n","    transform_fcns += list(aug_fcns)\n","    \n","    transform_fcns.append(transforms.ToTensor())\n","    transform_fcns.append(transforms.Normalize(MEAN_V, STD_V))\n","    \n","    return transforms.Compose(transform_fcns)\n","    "]},{"cell_type":"code","execution_count":5,"id":"b126f56c-212b-4512-b468-5ebc63eb3bc4","metadata":{"id":"b126f56c-212b-4512-b468-5ebc63eb3bc4"},"outputs":[],"source":["def load_cifar10(img_size: int = 32, \n","                 aug_fcns: Tuple = (), \n","                 validation_ratio: float = 0.8,\n","                ) -> Tuple[CIFAR10, CIFAR10, CIFAR10]:\n","    tf_train = generate_transform(resize=img_size, aug_fcns=aug_fcns)\n","    tf_test = generate_transform(resize=img_size)\n","    \n","    train_dataset = CIFAR10(root=DATASET_DIR, train=True, download=True, transform=tf_train)\n","    train_length = int(len(train_dataset) * validation_ratio)\n","    val_length = len(train_dataset) - train_length\n","\n","    train_dataset, val_dataset = random_split(train_dataset, [train_length, val_length])\n","    test_dataset = CIFAR10(root=DATASET_DIR, train=False, download=True, transform=tf_test)\n","    \n","    return train_dataset, val_dataset, test_dataset\n","    "]},{"cell_type":"code","execution_count":6,"id":"f0850dec-490d-44c8-a1b2-7b2bd0205d84","metadata":{"id":"f0850dec-490d-44c8-a1b2-7b2bd0205d84","outputId":"05d48bf7-802e-4346-9c48-77a8f420e770"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./input/cifar10/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dc62aa91e3b4ea5bcac6088256a1fb2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./input/cifar10/cifar-10-python.tar.gz to ./input/cifar10\n","Files already downloaded and verified\n"]}],"source":["img_size = 32\n","\n","augmentation_functions = (\n","    transforms.ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5)), \n","    transforms.RandomPerspective(),\n","    transforms.RandomHorizontalFlip(),\n",")\n","\n","train_dataset, val_dataset, test_dataset = load_cifar10(img_size=img_size, aug_fcns = augmentation_functions, validation_ratio=0.8)"]},{"cell_type":"code","execution_count":7,"id":"5076e220-769e-4e05-896a-5caabb17ebca","metadata":{"id":"5076e220-769e-4e05-896a-5caabb17ebca"},"outputs":[],"source":["def tensor_to_img(tensor_img: torch.Tensor) -> np.ndarray:\n","    return ((tensor_img.permute(1, 2, 0).numpy() * STD_V + MEAN_V) * 255).astype(np.uint8)\n","\n","def visualize_datasets(_train_dataset: CIFAR10, _val_dataset: CIFAR10, _test_dataset: CIFAR10, title_prefix: str = \"\") -> None:\n","    fig, ax = plt.subplots(3, 7, figsize=(20, 10))\n","\n","    for i in range(7):\n","        idx = np.random.randint(0, len(val_dataset))\n","\n","        ax[0][i].imshow(tensor_to_img(_train_dataset[idx][0]))\n","        ax[1][i].imshow(tensor_to_img(_val_dataset[idx][0]))\n","        ax[2][i].imshow(tensor_to_img(_test_dataset[idx][0]))\n","\n","        ax[0][i].axis('off')\n","        ax[1][i].axis('off')\n","        ax[2][i].axis('off')\n","\n","    fig.suptitle(f\"{title_prefix} Visualization of Augmentation.\\n(Each row represents train, validation, test dataset accordingly)\")\n","    fig.show()"]},{"cell_type":"code","execution_count":8,"id":"3c8a7c76-124b-411a-b5c1-a72e09b4de73","metadata":{"id":"3c8a7c76-124b-411a-b5c1-a72e09b4de73","outputId":"e5256769-8633-444d-f4bc-c0f38fdf5609"},"outputs":[],"source":["visualize_datasets(train_dataset, val_dataset, test_dataset, title_prefix=\"Trial 00 //\")"]},{"cell_type":"code","execution_count":9,"id":"051fde3e-1319-468f-bb3c-9fc2003be1b6","metadata":{"id":"051fde3e-1319-468f-bb3c-9fc2003be1b6"},"outputs":[],"source":["import yaml"]},{"cell_type":"code","execution_count":10,"id":"259ebabe-97d8-4280-9ce8-9559989d216d","metadata":{"id":"259ebabe-97d8-4280-9ce8-9559989d216d","outputId":"71d432ce-7061-416f-9662-fdca5d88af97"},"outputs":[{"name":"stdout","output_type":"stream","text":["idx |   n |     params |          module |            arguments |   in_channel |   out_channel\n","----------------------------------------------------------------------------------------------\n","  0 |   1 |        464 |            Conv | [16, 3, 2, None, 1, 'HardSwish'] |            3           16\n","  1 |   1 |        464 | InvertedResidualv3 |  [3, 1, 16, 0, 0, 1] |           16           16\n","  2 |   1 |      3,440 | InvertedResidualv3 |  [3, 4, 24, 0, 0, 2] |           16           24\n","  3 |   1 |      4,440 | InvertedResidualv3 |  [3, 3, 24, 0, 0, 1] |           24           24\n","  4 |   1 |     10,328 | InvertedResidualv3 |  [5, 3, 40, 1, 0, 2] |           24           40\n","  5 |   1 |     20,992 | InvertedResidualv3 |  [5, 3, 40, 1, 0, 1] |           40           40\n","  6 |   1 |     20,992 | InvertedResidualv3 |  [5, 3, 40, 1, 0, 1] |           40           40\n","  7 |   1 |     32,080 | InvertedResidualv3 |  [3, 6, 80, 0, 1, 2] |           40           80\n","  8 |   1 |     34,760 | InvertedResidualv3 | [3, 2.5, 80, 0, 1, 1] |           80           80\n","  9 |   1 |     31,992 | InvertedResidualv3 | [3, 2.3, 80, 0, 1, 1] |           80           80\n"," 10 |   1 |     31,992 | InvertedResidualv3 | [3, 2.3, 80, 0, 1, 1] |           80           80\n"," 11 |   1 |    214,424 | InvertedResidualv3 | [3, 6, 112, 1, 1, 1] |           80          112\n"," 12 |   1 |    386,120 | InvertedResidualv3 | [3, 6, 112, 1, 1, 1] |          112          112\n"," 13 |   1 |    429,224 | InvertedResidualv3 | [5, 6, 160, 1, 1, 2] |          112          160\n"," 14 |   1 |    797,360 | InvertedResidualv3 | [5, 6, 160, 1, 1, 1] |          160          160\n"," 15 |   1 |    797,360 | InvertedResidualv3 | [5, 6, 160, 1, 1, 1] |          160          160\n"," 16 |   1 |    155,520 |            Conv |          [960, 1, 1] |          160          960\n"," 17 |   1 |          0 |   GlobalAvgPool |                   [] |          960          960\n"," 18 |   1 |  1,231,360 |            Conv |         [1280, 1, 1] |          960         1280\n"," 19 |   1 |          0 |         Flatten |                   [] |         1280         1280\n"," 20 |   1 |     12,810 |          Linear |                 [10] |         1280           10\n","Model Summary: 227 layers, 4,216,122 parameters, 4,216,122 gradients\n"]}],"source":["with open(\"./configs/model/mobilenetv3.yaml\", \"r\") as f:\n","    model_cfg = yaml.load(f, yaml.SafeLoader)\n","\n","model_cfg['backbone'][-1][-1] = [10]\n","\n","model = Model(model_cfg, verbose=True)"]},{"cell_type":"code","execution_count":11,"id":"d3ea21cf-62be-4105-83ce-9366863e5030","metadata":{"id":"d3ea21cf-62be-4105-83ce-9366863e5030","outputId":"573db45e-b359-4ed0-8887-83dd07ab3844"},"outputs":[{"name":"stderr","output_type":"stream","text":["Train: [001] Loss: 2.156, Acc: 19.49% F1(macro): 0.19: 100%|██████████| 156/156 [00:10<00:00, 14.30it/s]\n"," Val:       Loss: 1.998, Acc: 26.18% F1(macro): 0.25: 100%|██████████| 40/40 [00:02<00:00, 15.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.247\n"]},{"name":"stderr","output_type":"stream","text":["Train: [002] Loss: 1.860, Acc: 31.36% F1(macro): 0.30: 100%|██████████| 156/156 [00:10<00:00, 14.41it/s]\n"," Val:       Loss: 1.784, Acc: 34.64% F1(macro): 0.34: 100%|██████████| 40/40 [00:02<00:00, 16.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.338\n"]},{"name":"stderr","output_type":"stream","text":["Train: [003] Loss: 1.726, Acc: 36.72% F1(macro): 0.36: 100%|██████████| 156/156 [00:10<00:00, 14.62it/s]\n"," Val:       Loss: 1.687, Acc: 38.66% F1(macro): 0.38: 100%|██████████| 40/40 [00:02<00:00, 14.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.376\n"]},{"name":"stderr","output_type":"stream","text":["Train: [004] Loss: 1.646, Acc: 39.77% F1(macro): 0.39: 100%|██████████| 156/156 [00:10<00:00, 14.68it/s]\n"," Val:       Loss: 1.627, Acc: 40.12% F1(macro): 0.39: 100%|██████████| 40/40 [00:02<00:00, 16.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.391\n"]},{"name":"stderr","output_type":"stream","text":["Train: [005] Loss: 1.594, Acc: 42.15% F1(macro): 0.42: 100%|██████████| 156/156 [00:10<00:00, 14.54it/s]\n"," Val:       Loss: 1.554, Acc: 43.50% F1(macro): 0.43: 100%|██████████| 40/40 [00:02<00:00, 17.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.426\n"]},{"name":"stderr","output_type":"stream","text":["Train: [006] Loss: 1.555, Acc: 43.57% F1(macro): 0.43: 100%|██████████| 156/156 [00:10<00:00, 14.18it/s]\n"," Val:       Loss: 1.546, Acc: 43.22% F1(macro): 0.42: 100%|██████████| 40/40 [00:02<00:00, 16.98it/s]\n","Train: [007] Loss: 1.508, Acc: 45.18% F1(macro): 0.45: 100%|██████████| 156/156 [00:10<00:00, 14.50it/s]\n"," Val:       Loss: 1.500, Acc: 45.12% F1(macro): 0.44: 100%|██████████| 40/40 [00:02<00:00, 17.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.442\n"]},{"name":"stderr","output_type":"stream","text":["Train: [008] Loss: 1.468, Acc: 46.92% F1(macro): 0.47: 100%|██████████| 156/156 [00:10<00:00, 14.29it/s]\n"," Val:       Loss: 1.461, Acc: 46.72% F1(macro): 0.46: 100%|██████████| 40/40 [00:02<00:00, 17.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.461\n"]},{"name":"stderr","output_type":"stream","text":["Train: [009] Loss: 1.426, Acc: 48.55% F1(macro): 0.48: 100%|██████████| 156/156 [00:10<00:00, 14.54it/s]\n"," Val:       Loss: 1.468, Acc: 47.35% F1(macro): 0.47: 100%|██████████| 40/40 [00:02<00:00, 16.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.473\n"]},{"name":"stderr","output_type":"stream","text":["Train: [010] Loss: 1.410, Acc: 49.49% F1(macro): 0.49: 100%|██████████| 156/156 [00:10<00:00, 14.47it/s]\n"," Val:       Loss: 1.431, Acc: 48.15% F1(macro): 0.47: 100%|██████████| 40/40 [00:02<00:00, 15.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Model saved. Current best test f1: 0.475\n"]}],"source":["EPOCHS = 10\n","BATCH_SIZE = 256\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = model.to(device)\n","\n","optimizer = torch.optim.SGD(model.model.parameters(), lr=0.1, momentum=0.9)\n","scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=0.1, steps_per_epoch=len(train_dataset), epochs=EPOCHS, pct_start=0.05)\n","criterion = CustomCriterion(samples_per_cls=None, device=device)\n","\n","train_loader = DataLoader(dataset=train_dataset, \n","                          pin_memory=torch.cuda.is_available(), \n","                          shuffle=True, \n","                          batch_size=BATCH_SIZE, \n","                          num_workers=4, \n","                          drop_last=True)\n","val_loader = DataLoader(dataset=val_dataset, \n","                        pin_memory=torch.cuda.is_available(), \n","                        shuffle=False, \n","                        batch_size=BATCH_SIZE, \n","                        num_workers=4)\n","test_loader = DataLoader(dataset=test_dataset, \n","                         pin_memory=torch.cuda.is_available(), \n","                         shuffle=False, \n","                         batch_size=BATCH_SIZE, \n","                         num_workers=4)\n","    \n","exp_dir = \"./exp/autoaug\"\n","os.makedirs(exp_dir, exist_ok=True)\n","trainer = TorchTrainer(model=model, \n","                       criterion=criterion, \n","                       optimizer=optimizer, \n","                       scheduler=scheduler, \n","                       device=device, \n","                       verbose=1, \n","                       model_path=os.path.join(exp_dir, \"best.pt\"))\n","\n","best_acc, best_f1 = trainer.train(train_dataloader=train_loader, \n","                                  n_epoch=EPOCHS, \n","                                  val_dataloader=val_loader)"]},{"cell_type":"code","execution_count":12,"id":"e604dbf5-e4d9-433f-97f3-29523d65208a","metadata":{"id":"e604dbf5-e4d9-433f-97f3-29523d65208a","outputId":"c947c3a9-7a28-4724-d7b7-67157e872044"},"outputs":[{"name":"stderr","output_type":"stream","text":[" Val:       Loss: 1.320, Acc: 53.17% F1(macro): 0.52: 100%|██████████| 40/40 [00:00<00:00, 40.17it/s]\n"]}],"source":["test_loss, test_f1, test_accuracy = trainer.test(model, test_loader)"]},{"cell_type":"code","execution_count":null,"id":"f83f9317","metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"RandAugmentation_vanilla.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":5}
